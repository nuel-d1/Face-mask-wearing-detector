{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NwQWLMu5JJxN"
   },
   "outputs": [],
   "source": [
    "#Imports here\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from zipfile import ZipFile\n",
    "from cv2 import resize\n",
    "from cv2.dnn import readNetFromCaffe, blobFromImage\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59r0DE-kClIu"
   },
   "outputs": [],
   "source": [
    "# Directory to save model checkpoint\n",
    "model_dir = 'models'\n",
    "\n",
    "# dataset root directory\n",
    "data_dir = 'mask_dataset'\n",
    "\n",
    "#directory to training dataset\n",
    "train_dir = data_dir + '/train'\n",
    "\n",
    "#directory to validation dataset\n",
    "valid_dir = data_dir + '/valid'\n",
    "\n",
    "#directory to testing dataset\n",
    "test_dir = data_dir + '/test'\n",
    "\n",
    "prototype_path = 'models/deploy.prototxt.txt'\n",
    "face_detection_model_path = 'models/res10_300x300_ssd_iter_140000.caffemodel'\n",
    "font = cv2.FONT_HERSHEY_DUPLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HTwqS_0ckeDh"
   },
   "outputs": [],
   "source": [
    "# Extracting data from zip file\n",
    "with ZipFile('mask_dataset.zip', 'r') as zipped_file:\n",
    "    zipped_file.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QD3RtgxdMJMY"
   },
   "outputs": [],
   "source": [
    "# transforms to be applied to training and validation dataset\n",
    "train_transforms = transforms.Compose([transforms.RandomPerspective(),\n",
    "                                       transforms.Resize((100,100)),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.4437, 0.3848, 0.3613], [0.2972, 0.2702, 0.2581])\n",
    "                                       ])\n",
    "\n",
    "# transforms to be applied to testing dataset\n",
    "test_transforms = transforms.Compose([transforms.Resize((100,100)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.4437, 0.3848, 0.3613], [0.2972, 0.2702, 0.2581])\n",
    "                                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndGHFY54JcSw"
   },
   "outputs": [],
   "source": [
    "# Loading training dataset\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "\n",
    "# Loading validation dataset\n",
    "valid_dataset = datasets.ImageFolder(valid_dir, transform=train_transforms)\n",
    "\n",
    "# Loading testing dataset\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mtczXuHVsFjn"
   },
   "outputs": [],
   "source": [
    "# calculating mean and standard deviation of dataset\n",
    "\n",
    "#mean = 0\n",
    "#std = 0\n",
    "#for images, _ in train_dataloader:\n",
    "#  batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "#  images = images.view(batch_samples, images.size(1), -1)\n",
    "#  mean += images.mean(2).sum(0)\n",
    "#  std += images.std(2).sum(0)\n",
    "\n",
    "#mean /= len(train_dataloader.dataset)\n",
    "#std /= len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgODBjOjJ3CF"
   },
   "outputs": [],
   "source": [
    "# Number of input data in a single batch\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Dataloader for training set\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Dataloader for validation set\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Dataloader for testing set\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "Z7JUI9nHOkJk",
    "outputId": "0a8983f1-8cf8-43e1-8388-0139fd2a760b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Confirm that training with gpu is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GR-RckXckhwf"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \"\"\"Convolutional Neural Network class\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.convLayer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.convLayer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.drop_out = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=(25 * 25 * 64), out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward pass\"\"\"\n",
    "\n",
    "        x = self.convLayer1(x)\n",
    "        x = self.convLayer2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PlATuyJ1OfRY"
   },
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "model.class_to_idx = train_dataset.class_to_idx\n",
    "model.to(device)\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPgpiNSBMtCx"
   },
   "outputs": [],
   "source": [
    "def validation():\n",
    "  \"\"\"function for validation of training results\"\"\"\n",
    "\n",
    "  test_loss = []\n",
    "  accuracy = []\n",
    "\n",
    "  for images, labels in valid_dataloader:\n",
    "    # moving tensors to gpu\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    #forward pass\n",
    "    output = model(images)\n",
    "    loss = criterion(output, labels)\n",
    "    test_loss.append(loss.item())\n",
    "\n",
    "    # calculating accuracy\n",
    "    total = labels.size(0)\n",
    "    _, prediction = torch.max(output.data, dim=1)\n",
    "    correct = (prediction == labels).sum().item()\n",
    "    accuracy.append(correct / total)\n",
    "\n",
    "  return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VNpr5x2gMpTS"
   },
   "outputs": [],
   "source": [
    "def training():\n",
    "  \"\"\"function for training model\"\"\"\n",
    "  epochs = 10\n",
    "  steps = 0\n",
    "  train_loss = []\n",
    "  print_count = 5\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    for images, labels in train_dataloader:\n",
    "      # forward pass\n",
    "      steps += 1\n",
    "      # moving tensors to gpu\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "      output = model(images)\n",
    "      loss = criterion(output, labels)\n",
    "      train_loss.append(loss.item())\n",
    "\n",
    "      # Backpropagation and optimization\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # validation\n",
    "      if steps % print_count == 0:\n",
    "        test_loss, accuracy = validation()\n",
    "\n",
    "        print('Epoch {}/{} | Training loss: {} | Test Loss: {} | Accuracy: {:.4f} %'\n",
    "        .format(epoch + 1, epochs, sum(train_loss) / BATCH_SIZE, \n",
    "                sum(test_loss) / len(valid_dataloader),\n",
    "                sum(accuracy) / len(valid_dataloader)))\n",
    "        \n",
    "        train_loss = []\n",
    "  print(\"\\nTraining process is now complete!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nKQo5IcwMvcA"
   },
   "outputs": [],
   "source": [
    "def testing():\n",
    "  \"\"\"function for testing model\"\"\"\n",
    "  with torch.no_grad():\n",
    "    steps = len(test_dataloader)\n",
    "    test_loss = []\n",
    "    accuracy = []\n",
    "    \n",
    "    for batch, (images, labels) in enumerate(test_dataloader):\n",
    "      # moving tensors to gpu\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      output = model(images)\n",
    "      loss = criterion(output, labels)\n",
    "      test_loss.append(loss.item())\n",
    "\n",
    "      # calculating accuracy\n",
    "      total = labels.size(0)\n",
    "      _, prediction = torch.max(output.data, dim=1)\n",
    "      correct = (prediction == labels).sum().item()\n",
    "      accuracy.append(correct / total)\n",
    "\n",
    "      print(\"batch {}\".format(batch + 1))\n",
    "      print(\"\\nPrediction accuracy ={:.1f}% \"\n",
    "      .format((sum(accuracy) / len(test_dataloader) * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dd83vEbyWbiE",
    "outputId": "697e5fa5-08f4-48ae-9420-8e4013cbf194"
   },
   "outputs": [],
   "source": [
    "# begin training\n",
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "SWlWJZmGWhz1",
    "outputId": "eba044c0-cdbb-4177-b79a-9125acad1e3c"
   },
   "outputs": [],
   "source": [
    "# begin testing\n",
    "testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZQqSgWfMx-i"
   },
   "outputs": [],
   "source": [
    "# Save the model state\n",
    "torch.save(model.state_dict(), model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BjcR4OBCbQ7"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (convLayer1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (convLayer2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (drop_out): Dropout(p=0.5, inplace=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=40000, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'detector_state.pth'\n",
    "\n",
    "def load_model(file_path):\n",
    "    \"\"\"function to load saved state of model\"\"\"\n",
    "    trained_model = ConvNet()\n",
    "    trained_model.load_state_dict(torch.load(file_path, map_location=torch.device('cpu')))\n",
    "    trained_model.eval()\n",
    "\n",
    "    return trained_model\n",
    "\n",
    "model  = load_model(PATH)\n",
    "model.class_to_idx = train_dataset.class_to_idx\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetector:\n",
    "    \"\"\" Class for the face detector model\"\"\"\n",
    "\n",
    "    def __init__(self, prototype, model):\n",
    "        self.prototype = prototype\n",
    "        self.model = model\n",
    "        self.confidence_threshold = 0.6\n",
    "        self.classifier = readNetFromCaffe(prototype, model)\n",
    "\n",
    "    def detect(self, image):\n",
    "        \"\"\"method to detect faces in input image\"\"\"\n",
    "        classifier = self.classifier\n",
    "        height, width = image.shape[:2]\n",
    "        image_blob = blobFromImage(resize(image, (300, 300)), 1.0, (300, 300), (103.93, 116.77, 123.68))\n",
    "        classifier.setInput(image_blob)\n",
    "        detections = classifier.forward()\n",
    "        faces = []\n",
    "        \n",
    "        # loop over the detections\n",
    "        for i in range(0, detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            # filter out weak detections by ensuring the 'confidence' is greater than the minimum confidence\n",
    "            if confidence > self.confidence_threshold:\n",
    "                # compute the coordinates of the bounding box for the object\n",
    "                box = detections[0, 0, i, 3:7] * np.array([width, height, width, height])\n",
    "                start_x, start_y, end_x, end_y = box.astype(\"int\")\n",
    "                # ensuring the bounding boxes fall within the dimensions of the frame\n",
    "                faces.append(np.array([start_x, start_y, end_x - start_x, end_y - start_y]))\n",
    "\n",
    "        return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    \"\"\"apply normalization to image for the pytorch model\"\"\"\n",
    "    try:\n",
    "        img = Image.fromarray(image)\n",
    "        transform = transforms.Compose([transforms.Resize((100, 100)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.4437, 0.3848, 0.3613], [0.2972, 0.2702, 0.2581])\n",
    "                                        ])\n",
    "        image = transform(img)\n",
    "        return image\n",
    "    except IOError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "    \"\"\"method to obtain predictions on passed images\"\"\"\n",
    "    with torch.no_grad():\n",
    "        classification = []\n",
    "        index_to_class = {value: key for key, value in model.class_to_idx.items()}\n",
    "        \n",
    "        #forward pass\n",
    "        image = process_image(images)\n",
    "        output = model(image[None])\n",
    "        label = output.numpy().argmax()\n",
    "        classification.append(index_to_class[label])\n",
    "        \n",
    "    return classification[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(frame, faces):\n",
    "    for (start_x, start_y, width, height) in faces:\n",
    "        # clamp coordinates that are outside of the image\n",
    "        start_x, start_y = max(start_x, 0), max(start_y, 0)\n",
    "        # obtain face coordinates\n",
    "        face_img = frame[start_y:start_y + height, start_x:start_x + width]\n",
    "        face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
    "        # make prediction\n",
    "        prediction = inference(face_img)\n",
    "        # create bounding box on face and add text label\n",
    "        label = prediction\n",
    "        if label == 'with_mask':\n",
    "            colour = (0, 255, 0) \n",
    "        elif label == 'without_mask':\n",
    "            colour = (0, 0, 255)\n",
    "            \n",
    "        cv2.rectangle(frame, (start_x, start_y), (start_x + width, start_y + height), colour, 1)\n",
    "        cv2.putText(frame, label, (start_x, start_y - 10), font, 0.5, colour, 2)\n",
    "        \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_prediction(image):\n",
    "    \"\"\"function that detects human faces in a given image and makes prediction on it\"\"\"\n",
    "    # load the model for face detection\n",
    "    face_detector = FaceDetector(prototype_path, face_detection_model_path)\n",
    "    # read input image\n",
    "    image = cv2.imread(image)\n",
    "    # detect faces in input image\n",
    "    faces = face_detector.detect(image)\n",
    "    # pass detected face for classification\n",
    "    classification(image, faces)\n",
    "    # display the output image\n",
    "    cv2.imshow('image', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prediction('trial images/test_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prediction('test/t7.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_prediction():\n",
    "    face_detector = FaceDetector(prototype_path, face_detection_model_path)\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    time.sleep(2.0)\n",
    "    \n",
    "    while True:\n",
    "        # capture frame by frame\n",
    "        ret, frame = video_capture.read()\n",
    "        faces = face_detector.detect(frame)\n",
    "        classification(frame, faces)\n",
    "        \n",
    "        # display the output image\n",
    "        cv2.imshow('Video', frame)\n",
    "        if cv2.waitKey(1) & 0xFF ==ord('q'):\n",
    "            break\n",
    "            \n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-0da432d3cc66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvideo_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-71-909c0958ea0d>\u001b[0m in \u001b[0;36mvideo_prediction\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m# capture frame by frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvideo_capture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mclassification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfaces\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-9f2a6f92a93f>\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;34m\"\"\"method to detect faces in input image\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mimage_blob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblobFromImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m103.93\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m116.77\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m123.68\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_blob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "video_prediction()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mask_detector.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
